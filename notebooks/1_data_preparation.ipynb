{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carbon Credit Projects in India: Data Preparation and Exploration\n",
    "\n",
    "**Author:** Anvith  \n",
    "**Date:** May 2025  \n",
    "**Version:** 1.0\n",
    "\n",
    "## Purpose\n",
    "This notebook performs the initial data loading, cleaning, and preparation steps for analyzing both Clean Development Mechanism (CDM) and Verified Carbon Standard (VCS) projects in India. The analysis examines determinants of success in emission reduction performance by comparing actual versus estimated emission reductions.\n",
    "\n",
    "## Input Data\n",
    "- `vcs_projects_for_analysis.csv`: VCS projects dataset with success indicators\n",
    "- `results_cdm.xlsx`: CDM projects dataset\n",
    "\n",
    "## Output Files\n",
    "- `vcs_processed.csv`: Processed VCS dataset with technology categories\n",
    "- `cdm_processed.csv`: Processed CDM dataset with matching structure\n",
    "- `combined_projects_categorized.csv`: Combined dataset for analysis\n",
    "\n",
    "## Workflow\n",
    "1. Load and process VCS data\n",
    "2. Load and process CDM data\n",
    "3. Standardize variables across both datasets\n",
    "4. Combine datasets for unified analysis\n",
    "\n",
    "## Dependencies\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set matplotlib to display plots inline\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Create directories for outputs if they don't exist\n",
    "os.makedirs('../output', exist_ok=True)\n",
    "os.makedirs('../output/figures', exist_ok=True)\n",
    "os.makedirs('../output/tables', exist_ok=True)\n",
    "\n",
    "# Helper function for file paths\n",
    "def get_path(file_name, directory=None):\n",
    "    \"\"\"\n",
    "    Get standardized file path.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_name : str\n",
    "        Name of the file\n",
    "    directory : str or None\n",
    "        Subdirectory (default: None, for root project directory)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Full file path\n",
    "    \"\"\"\n",
    "    if directory is None:\n",
    "        return file_name\n",
    "    else:\n",
    "        return os.path.join(directory, file_name)\n",
    "\n",
    "# Progress tracking function\n",
    "def log_progress(step, message):\n",
    "    \"\"\"Log progress with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"[{timestamp}] {step}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# PARAMETERS\n",
    "###################\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = '../data'  # Parent directory containing input data\n",
    "OUTPUT_DIR = '../output'  # Parent directory for output files\n",
    "FIGURES_DIR = os.path.join(OUTPUT_DIR, 'figures')\n",
    "TABLES_DIR = os.path.join(OUTPUT_DIR, 'tables')\n",
    "\n",
    "# Input and output files\n",
    "VCS_INPUT_FILE = os.path.join(DATA_DIR, 'vcs_projects_for_analysis.csv')\n",
    "CDM_INPUT_FILE = os.path.join(DATA_DIR, 'results_cdm.csv')\n",
    "VCS_PROCESSED_FILE = os.path.join(OUTPUT_DIR, 'vcs_processed.csv')\n",
    "CDM_PROCESSED_FILE = os.path.join(OUTPUT_DIR, 'cdm_processed.csv')\n",
    "COMBINED_OUTPUT = os.path.join(OUTPUT_DIR, 'combined_projects_categorized.csv')\n",
    "\n",
    "# Analysis parameters\n",
    "BASE_TECH_CATEGORY = 'Wind'  # Base technology for regression\n",
    "EMISSION_THRESHOLD = 60000  # Threshold for large-scale projects (tCO2e/year)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8')  # Use compatible style\n",
    "COLORS = sns.color_palette(\"viridis\", 8)\n",
    "sns.set_context(\"paper\", font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. VCS Data Processing\n",
    "\n",
    "This section loads the VCS dataset and performs basic categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VCS dataset\n",
    "log_progress(\"VCS Data Loading\", \"Starting to load data\")\n",
    "df_vcs = pd.read_csv(VCS_INPUT_FILE)\n",
    "log_progress(\"VCS Data Loading\", f\"Completed - loaded {len(df_vcs)} projects\")\n",
    "\n",
    "# Basic dataset information\n",
    "print(f\"VCS dataset contains {len(df_vcs)} projects with complete data\")\n",
    "print(f\"Number of columns: {len(df_vcs.columns)}\")\n",
    "print(\"\\nVCS Column names:\")\n",
    "for i, col in enumerate(df_vcs.columns):\n",
    "    print(f\"  {i}: {col}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of VCS data:\")\n",
    "print(df_vcs.head())\n",
    "\n",
    "# Summary statistics for key numeric columns\n",
    "print(\"\\nSummary statistics for key VCS metrics:\")\n",
    "numeric_cols = ['Estimated Annual Emission Reductions', 't_actual_years', \n",
    "                'Total_Actual_VCUs_Issued', 'q_quotient', 'log_q_success_indicator']\n",
    "print(df_vcs[numeric_cols].describe())\n",
    "\n",
    "# Add regime indicator\n",
    "df_vcs['Regime'] = 'VCS'\n",
    "\n",
    "# Create a function to categorize projects based on your data\n",
    "def categorize_technology(row):\n",
    "    \"\"\"\n",
    "    Categorize projects into key technology types based on Project Type and Methodology\n",
    "    \"\"\"\n",
    "    project_type = str(row['Project Type']).lower()\n",
    "    methodology = str(row['Methodology']).lower()\n",
    "    \n",
    "    # Wind projects\n",
    "    if any(term in project_type for term in ['wind', 'eolic']):\n",
    "        return 'Wind'\n",
    "    \n",
    "    # Hydro projects\n",
    "    elif any(term in project_type for term in ['hydro', 'hydroelectric', 'hydropower']):\n",
    "        return 'Hydro'\n",
    "    \n",
    "    # Solar projects\n",
    "    elif any(term in project_type for term in ['solar', 'photovoltaic', 'pv']):\n",
    "        return 'Solar'\n",
    "    \n",
    "    # Biomass and waste-to-energy projects\n",
    "    elif any(term in project_type for term in ['biomass', 'biogas', 'landfill', 'methane', 'waste']):\n",
    "        return 'Biomass and Waste'\n",
    "    \n",
    "    # Energy efficiency projects, including cookstoves\n",
    "    elif any(term in project_type for term in ['efficiency', 'efficient', 'cookstove', 'cook stove', 'demand']):\n",
    "        return 'Energy Efficiency'\n",
    "    \n",
    "    # HFC/Industrial gas projects (more common in CDM)\n",
    "    elif any(term in project_type for term in ['hfc', 'industrial gas', 'n2o', 'pfc', 'sf6']):\n",
    "        return 'Industrial Gases'\n",
    "    \n",
    "    # Use methodology as a fallback if project type is unclear\n",
    "    elif 'ams-i' in methodology:  # Small-scale renewable energy\n",
    "        return 'Other Renewable'\n",
    "    elif 'ams-ii' in methodology:  # Small-scale energy efficiency\n",
    "        return 'Energy Efficiency'\n",
    "    elif 'ams-iii' in methodology:  # Small-scale other project activities\n",
    "        return 'Other'\n",
    "    elif 'acm000' in methodology:  # Large-scale consolidated methodologies\n",
    "        return 'Other'\n",
    "    \n",
    "    # Default category\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the technology categorization function to VCS data\n",
    "df_vcs['Technology_Category'] = df_vcs.apply(categorize_technology, axis=1)\n",
    "\n",
    "# Check the distribution of categories\n",
    "tech_distribution_vcs = df_vcs['Technology_Category'].value_counts()\n",
    "print(\"\\nVCS Technology Categories:\")\n",
    "print(tech_distribution_vcs)\n",
    "\n",
    "# Create dummy variables for regression (Wind as base category)\n",
    "tech_dummies_vcs = pd.get_dummies(df_vcs['Technology_Category'], prefix='Tech')\n",
    "\n",
    "# Check if base category exists before dropping\n",
    "if f'Tech_{BASE_TECH_CATEGORY}' in tech_dummies_vcs.columns:\n",
    "    tech_dummies_vcs = tech_dummies_vcs.drop(f'Tech_{BASE_TECH_CATEGORY}', axis=1)  # Wind as base category\n",
    "    print(f\"\\nBase category for Technology: {BASE_TECH_CATEGORY}\")\n",
    "else:\n",
    "    # If no Wind projects, use another category as base\n",
    "    base_category = tech_distribution_vcs.index[0]  # Most common category\n",
    "    tech_dummies_vcs = tech_dummies_vcs.drop(f'Tech_{base_category}', axis=1)\n",
    "    print(f\"\\nBase category for Technology: {base_category}\")\n",
    "\n",
    "# Add dummy variables to the dataset\n",
    "df_vcs = pd.concat([df_vcs, tech_dummies_vcs], axis=1)\n",
    "\n",
    "# Determine scale for VCS projects\n",
    "def determine_scale(row):\n",
    "    \"\"\"\n",
    "    Determine if a project is small scale or large scale based on methodology and size\n",
    "    \"\"\"\n",
    "    methodology = str(row['Methodology']).upper()\n",
    "    emissions = row['Estimated Annual Emission Reductions']\n",
    "    \n",
    "    # Clear indicators from methodology\n",
    "    if 'AMS-' in methodology:  # AMS = Small-scale\n",
    "        return 'Small Scale'\n",
    "    elif 'ACM' in methodology or 'AM' in methodology:  # ACM/AM = Large-scale\n",
    "        return 'Large Scale'\n",
    "    \n",
    "    # Use emission size as fallback\n",
    "    elif emissions > EMISSION_THRESHOLD:  # Common threshold\n",
    "        return 'Large Scale'\n",
    "    else:\n",
    "        return 'Small Scale'\n",
    "\n",
    "# Apply the scale determination function to VCS data\n",
    "df_vcs['Scale_Category'] = df_vcs.apply(determine_scale, axis=1)\n",
    "\n",
    "# Create a binary indicator for regression (Small Scale as base)\n",
    "df_vcs['Scale_Large'] = df_vcs['Scale_Category'].apply(lambda x: 1 if x == 'Large Scale' else 0)\n",
    "\n",
    "# Check the distribution of scale categories for VCS\n",
    "scale_distribution_vcs = df_vcs['Scale_Category'].value_counts()\n",
    "print(\"\\nVCS Scale Categories:\")\n",
    "print(scale_distribution_vcs)\n",
    "\n",
    "# Determine international participation for VCS projects\n",
    "def has_international_participation(proponent):\n",
    "    \"\"\"\n",
    "    Identify if a project likely has international participation based on proponent name\n",
    "    \"\"\"\n",
    "    if pd.isna(proponent):\n",
    "        return 0\n",
    "    \n",
    "    proponent = str(proponent).lower()\n",
    "    \n",
    "    # Keywords suggesting international entities\n",
    "    int_keywords = [\n",
    "        'gmbh', 'ltd', 'inc', 'international', 'global', 'europe', 'japan', \n",
    "        'usa', 'america', 'trading', 'capital', 'carbon', 'markets', 'ag', \n",
    "        'b.v.', 'holding', 'energy', 'investments', 'asset', 'management',\n",
    "        'fund', 'green', 'climate', 'development', 'financing', 'switzerland',\n",
    "        'uk', 'spain', 'germany', 'netherlands', 'france', 'denmark', 'sweden',\n",
    "        'zurich', 'london', 'world', 'bank', 'corp', 'group'\n",
    "    ]\n",
    "    \n",
    "    # Indian-specific terms that don't indicate international participation\n",
    "    indian_terms = [\n",
    "        'india', 'indian', 'limited', 'pvt', 'private', 'bharat', 'infra', \n",
    "        'power', 'maharashtra', 'gujarat', 'tamil', 'nadu', 'karnataka', \n",
    "        'pradesh', 'punjab', 'delhi', 'mumbai', 'bangalore', 'chennai', \n",
    "        'kolkata', 'hyderabad', 'renewables', 'energy', 'solar', 'wind'\n",
    "    ]\n",
    "    \n",
    "    # Check for international keywords not also containing Indian terms\n",
    "    for keyword in int_keywords:\n",
    "        if keyword in proponent and not any(term in proponent for term in indian_terms):\n",
    "            return 1\n",
    "    \n",
    "    # Special case: If it explicitly mentions partnership/joint venture\n",
    "    if any(term in proponent for term in ['partnership', 'joint venture', 'collaboration']):\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# Apply the international participation function to VCS data\n",
    "df_vcs['Is_International'] = df_vcs['Proponent'].apply(has_international_participation)\n",
    "\n",
    "# Check the distribution for VCS\n",
    "int_distribution_vcs = df_vcs['Is_International'].value_counts()\n",
    "print(\"\\nVCS International Participation:\")\n",
    "print(int_distribution_vcs)\n",
    "print(f\"Percentage with international participation: {df_vcs['Is_International'].mean()*100:.1f}%\")\n",
    "\n",
    "# Save the processed VCS dataset\n",
    "df_vcs.to_csv(VCS_PROCESSED_FILE, index=False)\n",
    "log_progress(\"VCS Processing\", f\"Completed - saved to {VCS_PROCESSED_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CDM Data Processing\n",
    "\n",
    "This section loads and processes the CDM dataset to match the VCS structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CDM dataset\n",
    "log_progress(\"CDM Data Loading\", \"Starting to load data\")\n",
    "df_cdm_raw = pd.read_csv(CDM_INPUT_FILE)\n",
    "\n",
    "log_progress(\"CDM Data Loading\", f\"Completed - loaded {len(df_cdm_raw)} projects\")\n",
    "df_cdm_raw.columns = df_cdm_raw.columns.str.strip()\n",
    "\n",
    "# Basic dataset information\n",
    "print(f\"CDM dataset contains {len(df_cdm_raw)} projects\")\n",
    "print(f\"Number of columns: {len(df_cdm_raw.columns)}\")\n",
    "\n",
    "# Display first few rows of critical columns\n",
    "critical_columns = [\n",
    "   'CDM project reference number',\n",
    "   'Registration project title',\n",
    "   'Project classification',\n",
    "   'Methodologies used at registration',\n",
    "   'Project type (UNEP DTU)',\n",
    "   'Project subtype (UNEP DTU)',\n",
    "   'Start of first crediting period',\n",
    "   'End of first crediting period',\n",
    "   'Amount Of Reductions (PDD ex-ante) per year in CP1 (PoA based on the sum of curr',\n",
    "   'First issuance',\n",
    "   'Lastest issuance',\n",
    "   'Total CERs issued'\n",
    "]\n",
    "\n",
    "# Check if all critical columns exist\n",
    "missing_cols = [col for col in critical_columns if col not in df_cdm_raw.columns]\n",
    "if missing_cols:\n",
    "   print(f\"WARNING: Missing critical columns in CDM data: {missing_cols}\")\n",
    "else:\n",
    "   print(\"All critical columns present in CDM data\")\n",
    "\n",
    "# Display sample of critical columns\n",
    "print(\"\\nSample CDM data (critical columns):\")\n",
    "sample_data = df_cdm_raw[critical_columns].head(3)\n",
    "for col in critical_columns:\n",
    "   print(f\"\\n{col}:\")\n",
    "   print(sample_data[col])\n",
    "\n",
    "# Process CDM data to match VCS structure\n",
    "log_progress(\"CDM Processing\", \"Standardizing CDM data structure\")\n",
    "\n",
    "# Create a new DataFrame for standardized CDM data\n",
    "df_cdm = pd.DataFrame()\n",
    "\n",
    "# Basic project information\n",
    "df_cdm['ID'] = df_cdm_raw['CDM project reference number'].astype(str)\n",
    "df_cdm['Name'] = df_cdm_raw['Registration project title']\n",
    "df_cdm['Proponent'] = df_cdm_raw['DOE']  # Use DOE as a proxy for proponent\n",
    "df_cdm['Project Type'] = (df_cdm_raw['Project type (UNEP DTU)'].fillna('') + ': ' + \n",
    "                         df_cdm_raw['Project subtype (UNEP DTU)'].fillna(''))\n",
    "df_cdm['AFOLU Activities'] = np.nan  # Not typically relevant for CDM\n",
    "df_cdm['Methodology'] = df_cdm_raw['Methodologies used at registration']\n",
    "df_cdm['Status'] = df_cdm_raw['Website project status']\n",
    "df_cdm['Country/Area'] = 'India'  # All projects are in India\n",
    "\n",
    "# Clean and convert estimated emission reductions (handle commas and formatting)\n",
    "emission_col = 'Amount Of Reductions (PDD ex-ante) per year in CP1 (PoA based on the sum of curr'\n",
    "df_cdm['Estimated Annual Emission Reductions'] = df_cdm_raw[emission_col].astype(str).str.replace(',', '')\n",
    "df_cdm['Estimated Annual Emission Reductions'] = pd.to_numeric(df_cdm['Estimated Annual Emission Reductions'], errors='coerce')\n",
    "\n",
    "# Print sample of estimated reductions for debugging\n",
    "print(\"\\nSample of estimated annual emission reductions:\")\n",
    "print(df_cdm['Estimated Annual Emission Reductions'].head())\n",
    "\n",
    "df_cdm['Region'] = 'ASP'  # Asia-Pacific region\n",
    "\n",
    "# Date fields - convert to datetime with explicit format\n",
    "# Use a function to try multiple date formats\n",
    "def parse_date(date_str):\n",
    "   \"\"\"Try to parse date string with multiple formats\"\"\"\n",
    "   if pd.isna(date_str):\n",
    "       return pd.NaT\n",
    "   \n",
    "   date_str = str(date_str).strip()\n",
    "   \n",
    "   # Try different date formats\n",
    "   formats = [\n",
    "       '%d-%m-%y', '%d-%m-%Y',  # DD-MM-YY, DD-MM-YYYY\n",
    "       '%Y-%m-%d',  # YYYY-MM-DD\n",
    "       '%d/%m/%y', '%d/%m/%Y',  # DD/MM/YY, DD/MM/YYYY\n",
    "       '%m-%d-%y', '%m-%d-%Y',  # MM-DD-YY, MM-DD-YYYY\n",
    "       '%m/%d/%y', '%m/%d/%Y'   # MM/DD/YY, MM/DD/YYYY\n",
    "   ]\n",
    "   \n",
    "   for fmt in formats:\n",
    "       try:\n",
    "           return pd.to_datetime(date_str, format=fmt)\n",
    "       except:\n",
    "           continue\n",
    "   \n",
    "   # If all explicit formats fail, try with dateutil parser\n",
    "   try:\n",
    "       return pd.to_datetime(date_str)\n",
    "   except:\n",
    "       return pd.NaT\n",
    "\n",
    "# Apply date parsing\n",
    "df_cdm['Project Registration Date'] = df_cdm_raw['Date of EB decision (see EB59 Annex 12, §25)'].apply(parse_date)\n",
    "df_cdm['Crediting Period Start Date'] = df_cdm_raw['Start of first crediting period'].apply(parse_date)\n",
    "df_cdm['Crediting Period End Date'] = df_cdm_raw['End of first crediting period'].apply(parse_date)\n",
    "\n",
    "# Clean and convert actual issuance data\n",
    "# Handle comma-separated numbers in 'Total CERs issued'\n",
    "df_cdm['Total_Actual_VCUs_Issued'] = df_cdm_raw['Total CERs issued'].astype(str).str.replace(',', '')\n",
    "df_cdm['Total_Actual_VCUs_Issued'] = pd.to_numeric(df_cdm['Total_Actual_VCUs_Issued'], errors='coerce')\n",
    "\n",
    "# Print sample of actual issuance for debugging\n",
    "print(\"\\nSample of total actual CERs issued:\")\n",
    "print(df_cdm['Total_Actual_VCUs_Issued'].head())\n",
    "\n",
    "# Parse issuance dates\n",
    "df_cdm['Actual_Issuance_Start_Date'] = df_cdm_raw['First issuance'].apply(parse_date)\n",
    "df_cdm['Actual_Issuance_End_Date'] = df_cdm_raw['Lastest issuance'].apply(parse_date)\n",
    "\n",
    "# Print sample of issuance dates for debugging\n",
    "print(\"\\nSample of issuance dates:\")\n",
    "print(pd.DataFrame({\n",
    "   'First': df_cdm['Actual_Issuance_Start_Date'].head(),\n",
    "   'Last': df_cdm['Actual_Issuance_End_Date'].head()\n",
    "}))\n",
    "\n",
    "# Calculate actual days/years\n",
    "# Only calculate for rows where both dates are available\n",
    "valid_dates_mask = (df_cdm['Actual_Issuance_Start_Date'].notna() & \n",
    "                   df_cdm['Actual_Issuance_End_Date'].notna())\n",
    "\n",
    "print(f\"\\nProjects with valid issuance dates: {valid_dates_mask.sum()} of {len(df_cdm)}\")\n",
    "\n",
    "# Initialize with NaN\n",
    "df_cdm['t_actual_days'] = np.nan\n",
    "df_cdm['t_actual_years'] = np.nan\n",
    "\n",
    "# Calculate only for valid rows\n",
    "df_cdm.loc[valid_dates_mask, 't_actual_days'] = (\n",
    "   df_cdm.loc[valid_dates_mask, 'Actual_Issuance_End_Date'] - \n",
    "   df_cdm.loc[valid_dates_mask, 'Actual_Issuance_Start_Date']\n",
    ").dt.days\n",
    "\n",
    "# Filter for positive durations\n",
    "positive_days_mask = df_cdm['t_actual_days'] > 0\n",
    "df_cdm.loc[positive_days_mask, 't_actual_years'] = df_cdm.loc[positive_days_mask, 't_actual_days'] / 365.25\n",
    "\n",
    "print(f\"Projects with positive duration: {positive_days_mask.sum()} of {valid_dates_mask.sum()}\")\n",
    "\n",
    "# Calculate performance metrics\n",
    "# Only calculate for rows with all required values\n",
    "valid_metrics_mask = (\n",
    "   df_cdm['Estimated Annual Emission Reductions'].notna() & \n",
    "   df_cdm['Total_Actual_VCUs_Issued'].notna() & \n",
    "   df_cdm['t_actual_years'].notna() & \n",
    "   (df_cdm['Estimated Annual Emission Reductions'] > 0) &\n",
    "   (df_cdm['t_actual_years'] > 0)\n",
    ")\n",
    "\n",
    "print(f\"\\nProjects with all required values for metrics: {valid_metrics_mask.sum()} of {len(df_cdm)}\")\n",
    "\n",
    "# Initialize with NaN\n",
    "df_cdm['Total_Estimated_ERs_Actual_Period'] = np.nan\n",
    "df_cdm['q_quotient'] = np.nan\n",
    "df_cdm['log_q_success_indicator'] = np.nan\n",
    "\n",
    "# Calculate only for valid rows\n",
    "df_cdm.loc[valid_metrics_mask, 'Total_Estimated_ERs_Actual_Period'] = (\n",
    "   df_cdm.loc[valid_metrics_mask, 'Estimated Annual Emission Reductions'] * \n",
    "   df_cdm.loc[valid_metrics_mask, 't_actual_years']\n",
    ")\n",
    "\n",
    "df_cdm.loc[valid_metrics_mask, 'q_quotient'] = (\n",
    "   df_cdm.loc[valid_metrics_mask, 'Total_Actual_VCUs_Issued'] / \n",
    "   df_cdm.loc[valid_metrics_mask, 'Total_Estimated_ERs_Actual_Period']\n",
    ")\n",
    "\n",
    "# Only calculate log for positive q_quotient\n",
    "positive_q_mask = (df_cdm['q_quotient'] > 0)\n",
    "df_cdm.loc[positive_q_mask, 'log_q_success_indicator'] = np.log(df_cdm.loc[positive_q_mask, 'q_quotient'])\n",
    "\n",
    "print(f\"Projects with valid success indicators: {df_cdm['log_q_success_indicator'].notna().sum()} of {len(df_cdm)}\")\n",
    "\n",
    "# Print distribution of success indicators for debugging\n",
    "valid_log_q = df_cdm['log_q_success_indicator'].dropna()\n",
    "if len(valid_log_q) > 0:\n",
    "   print(\"\\nDistribution of log_q_success_indicator for CDM projects:\")\n",
    "   print(valid_log_q.describe())\n",
    "\n",
    "# Add regime indicator\n",
    "df_cdm['Regime'] = 'CDM'\n",
    "\n",
    "# Apply technology categorization function to CDM data\n",
    "df_cdm['Technology_Category'] = df_cdm.apply(categorize_technology, axis=1)\n",
    "\n",
    "# Check the distribution of categories for CDM\n",
    "tech_distribution_cdm = df_cdm['Technology_Category'].value_counts()\n",
    "print(\"\\nCDM Technology Categories:\")\n",
    "print(tech_distribution_cdm)\n",
    "\n",
    "# Create dummy variables for regression\n",
    "tech_dummies_cdm = pd.get_dummies(df_cdm['Technology_Category'], prefix='Tech')\n",
    "if f'Tech_{BASE_TECH_CATEGORY}' in tech_dummies_cdm.columns:\n",
    "   tech_dummies_cdm = tech_dummies_cdm.drop(f'Tech_{BASE_TECH_CATEGORY}', axis=1)  # Wind as base\n",
    "df_cdm = pd.concat([df_cdm, tech_dummies_cdm], axis=1)\n",
    "\n",
    "# Determine scale for CDM projects (based on project classification)\n",
    "df_cdm['Scale_Category'] = df_cdm_raw['Project classification'].fillna('SMALL')\n",
    "df_cdm['Scale_Large'] = df_cdm['Scale_Category'].str.upper().eq('LARGE').astype(int)\n",
    "\n",
    "# Check the scale distribution for CDM\n",
    "scale_distribution_cdm = df_cdm['Scale_Category'].value_counts()\n",
    "print(\"\\nCDM Scale Categories:\")\n",
    "print(scale_distribution_cdm)\n",
    "\n",
    "# For CDM, all projects involve international participation through the CDM framework\n",
    "df_cdm['Is_International'] = 1\n",
    "\n",
    "# Check missing values in key fields\n",
    "missing_vals_cdm = df_cdm[['log_q_success_indicator', 't_actual_years', 'Total_Actual_VCUs_Issued']].isnull().sum()\n",
    "print(\"\\nMissing values in key CDM columns:\")\n",
    "print(missing_vals_cdm)\n",
    "\n",
    "# Filter for CDM projects with valid success indicators\n",
    "df_cdm_valid = df_cdm.dropna(subset=['log_q_success_indicator'])\n",
    "print(f\"\\nCDM projects with valid success indicators: {len(df_cdm_valid)} of {len(df_cdm)}\")\n",
    "\n",
    "# Save the processed CDM dataset\n",
    "df_cdm.to_csv(CDM_PROCESSED_FILE, index=False)\n",
    "log_progress(\"CDM Processing\", f\"Completed - saved to {CDM_PROCESSED_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combined Dataset Creation\n",
    "\n",
    "This section combines the processed VCS and CDM datasets for unified analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine VCS and CDM datasets\n",
    "log_progress(\"Data Combination\", \"Combining VCS and CDM datasets\")\n",
    "\n",
    "# Create combined dataset\n",
    "df_combined = pd.concat([df_vcs, df_cdm], ignore_index=True)\n",
    "\n",
    "# Check the combined dataset\n",
    "print(f\"Combined dataset contains {len(df_combined)} projects\")\n",
    "print(f\"  VCS projects: {(df_combined['Regime'] == 'VCS').sum()}\")\n",
    "print(f\"  CDM projects: {(df_combined['Regime'] == 'CDM').sum()}\")\n",
    "\n",
    "# Check for missing values in key columns\n",
    "missing_vals_combined = df_combined[['log_q_success_indicator', 't_actual_years', 'Regime']].isnull().sum()\n",
    "print(\"\\nMissing values in key columns of combined dataset:\")\n",
    "print(missing_vals_combined)\n",
    "\n",
    "# Create a variable to indicate CDM projects (for regression)\n",
    "df_combined['Is_CDM'] = (df_combined['Regime'] == 'CDM').astype(int)\n",
    "\n",
    "# Filter for projects with valid success indicators\n",
    "df_combined_valid = df_combined.dropna(subset=['log_q_success_indicator'])\n",
    "print(f\"\\nProjects with valid success indicators: {len(df_combined_valid)} of {len(df_combined)}\")\n",
    "print(f\"  VCS projects: {(df_combined_valid['Regime'] == 'VCS').sum()}\")\n",
    "print(f\"  CDM projects: {(df_combined_valid['Regime'] == 'CDM').sum()}\")\n",
    "\n",
    "# Compare success indicators between regimes\n",
    "success_by_regime = df_combined_valid.groupby('Regime')['log_q_success_indicator'].agg(['count', 'mean', 'median', 'std'])\n",
    "print(\"\\nSuccess indicator by regime:\")\n",
    "print(success_by_regime)\n",
    "\n",
    "# Save the combined dataset\n",
    "df_combined.to_csv(COMBINED_OUTPUT, index=False)\n",
    "log_progress(\"Data Combination\", f\"Saved combined dataset with {len(df_combined)} projects\")\n",
    "\n",
    "# Create visualizations comparing regimes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Regime', y='log_q_success_indicator', data=df_combined_valid)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Success Indicator (log(q)) by Regime')\n",
    "plt.xlabel('Regime')\n",
    "plt.ylabel('log(q)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'success_by_regime.png'), dpi=300)\n",
    "\n",
    "# Technology distribution by regime\n",
    "tech_by_regime = pd.crosstab(df_combined_valid['Regime'], df_combined_valid['Technology_Category'])\n",
    "tech_by_regime_pct = tech_by_regime.div(tech_by_regime.sum(axis=1), axis=0) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "tech_by_regime_pct.plot(kind='bar', stacked=True)\n",
    "plt.title('Technology Distribution by Regime')\n",
    "plt.xlabel('Regime')\n",
    "plt.ylabel('Percentage')\n",
    "plt.legend(title='Technology Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'technology_by_regime.png'), dpi=300)\n",
    "\n",
    "# Scale distribution by regime\n",
    "scale_by_regime = pd.crosstab(df_combined_valid['Regime'], df_combined_valid['Scale_Category'])\n",
    "scale_by_regime_pct = scale_by_regime.div(scale_by_regime.sum(axis=1), axis=0) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scale_by_regime_pct.plot(kind='bar', stacked=True)\n",
    "plt.title('Scale Distribution by Regime')\n",
    "plt.xlabel('Regime')\n",
    "plt.ylabel('Percentage')\n",
    "plt.legend(title='Scale Category')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'scale_by_regime.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Outcomes\n",
    "\n",
    "This notebook has successfully:\n",
    "\n",
    "1. **Processed VCS Data**: Loaded and categorized VCS projects from India\n",
    "2. **Processed CDM Data**: Standardized CDM project data to match VCS structure\n",
    "3. **Created Technology Categories**: Applied consistent technology classification across both regimes\n",
    "4. **Added Scale Variables**: Identified small vs. large scale projects\n",
    "5. **Added Participation Information**: Determined international participation\n",
    "6. **Combined Datasets**: Created a unified dataset with both CDM and VCS projects\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "The combined dataset has been saved as 'combined_projects_categorized.csv' and is ready for:\n",
    "\n",
    "1. Descriptive statistical analysis (Notebook 2)\n",
    "2. Regression analysis to identify success determinants (Notebook 3)\n",
    "\n",
    "Key questions to explore next:\n",
    "- How do CDM and VCS projects compare in terms of success?\n",
    "- What factors influence success in each regime?\n",
    "- Are there significant differences in determinants between regimes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
